{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PracticalAssignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YqdW05iuaJDW",
        "9Xue1NVSA3WS",
        "fGv7az_y6Wyk",
        "oRxDShbjAXjI",
        "slmjGSiuFPZv",
        "k_DDcDPx_zKo",
        "rBrANeu0aILI",
        "0akOo4P-EjhV",
        "zxGXZkGXbnmJ",
        "HCAFmjmFd2U0",
        "ciF6EYBIeciU",
        "gTt_8MS4Zm03",
        "lRzDZxIhn76D"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunmenonv/EE5180/blob/main/PracticalAssignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqdW05iuaJDW"
      },
      "source": [
        "# Problem 1: Bayes Classifier\n",
        "\n",
        "Bayes classifiers fall under the class of **generative classifiers**. Generative classifiers attempt to learn the generation process of a dataset, usually by making some assumptions about the process that generates the data. Then such classifiers use the learned model to make a prediction or classify the unseen data. A simple example is a Naïve Bayes Classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHh1lpZBiblL"
      },
      "source": [
        "### Naïve Bayes classifier\n",
        "Consider a dataset $\\left\\{X^{(i)}, Y^{(i)}\\right\\}_{i=1}^{m}$. Each $X^{(i)}$ is an $n-$dimensional vector of input features. Let $Y^{(i)} \\in \\{0,1\\}$ denote the class to which $X^{(i)}$ belongs (this can be easily extended to multi-class problems as well). A good classifier has to accurately predict the probability that any given input $X$ falls in class $1$ which is $ P(Y=1 | X)$. \n",
        "\n",
        "Recall Bayes theorem,\n",
        "\n",
        "\\begin{align}\n",
        "P(Y|X) &= \\frac{P(X|Y)P(Y)}{P(X)} \\\\\n",
        "       &= \\frac{P(X_1, X_2, \\dots, X_n | Y)P(Y)}{P(X_1, X_2, \\dots, X_n)}\\\\\n",
        "\\end{align}\n",
        "\n",
        "**We use the assumption that features are independent of each other. That is one particular feature does not affect any other feature. Of course these assumptions of independence are rarely true, which is why the model is referred as the \"Naïve Bayes\" model. However, in practice, Naïve Bayes models have performed surprisingly well even on complex tasks, where it is clear that the strong independence assumptions are false.**\n",
        "\n",
        "The independence assumption reduces the conditional probability expression to\n",
        "\\begin{align}\n",
        "P(Y|X) &= \\frac{P(X_1 | Y)P(X_2 | Y) \\dots P(X_n | Y)P(Y)}{P(X_1)P(X_2)\\dots P(X_n)}\\\\\n",
        "\\end{align}\n",
        "\n",
        "The terms $P(X_i|Y)$ and $P(X_i)$ can be easily estimated/learned from the dataset. Hence, the value of $P(Y|X)$ can be found for each value of $Y$. Finally, the class to which $X$ belongs is estimated as $arg\\max_{Y}P(Y|X)$. Moreover since $X$ is independent of $Y$, it is only required to find $arg\\max_{Y}P(X|Y)P(Y).$ For better understanding with an example refer [this](https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c) article.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xue1NVSA3WS"
      },
      "source": [
        "### Problem statement and Dataset\n",
        "In this problem, you would implement, train and test a Naïve Bayes model to learn to classify sentiment (positive/negative) of a given text. The training data is in `all_sentiment_shuffled.txt` file.  You can use the function given below to read the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyeDf2Vn4xDW"
      },
      "source": [
        "def read_corpus(corpus_file):\n",
        "    \"\"\" This function reads the file in the location specified by string \n",
        "    `corpus_file` and returns a list of tuples (list of words in text, label)\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    with open(corpus_file) as f:\n",
        "        for line in f:\n",
        "            tokens = line.strip().split()\n",
        "            out.append((tokens[3:], tokens[1]))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA3_sL7OAkF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "0f2142a8-9da8-4cf6-963a-876414ff6d30"
      },
      "source": [
        "corpus = read_corpus('./all_sentiment_shuffled.txt')\n",
        "print(\"Example:\\n\", \" Text: \", corpus[0][0], \"\\n  Label: \", corpus[0][1])\n",
        "print(\"Total number of documents =\", len(corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-660772006ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./all_sentiment_shuffled.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Example:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Text: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n  Label: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of documents =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-cb90fa9f821a>\u001b[0m in \u001b[0;36mread_corpus\u001b[0;34m(corpus_file)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './all_sentiment_shuffled.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGv7az_y6Wyk"
      },
      "source": [
        "### Preprocessing a text document\n",
        "We can guess that not all the words in a document will be helpful in classification. The words such as \"a\", \"the\", \"is\", etc appear in all the documents randomly and can be neglected or removed. Also a same word can be written in different tenses while conveying the same mood (example \"rot\"/\"rotten\"). Hence the documents need to be preprocessed before using them for training the classifier.\n",
        "\n",
        " Libraries such as `gensim`, `nltk` contain functions for doing these preprocessing steps, and you are welcome to use such functions in your code. Formally, these are the preprocessings to be done to the input text to make them simpler and which can improve the performance of your model as well.\n",
        "* **Tokenization**: \n",
        "    1.   Split the text into sentences and the sentences into words\n",
        "    2.   Lowercase the words and remove punctuation\n",
        "* Remove all **stopwords** (stopwords are commonly used word such as \"the\", \"a\", \"an\", \"in\")\n",
        "* Remove all words that have fewer than 3 characters.\n",
        "* **Lemmatize** the document (words in third person are changed to first person, and verbs in past and future tenses are changed into present).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtSfxXMwZj8J"
      },
      "source": [
        "\"\"\" Implement preprocessing functions here. Use the python modules named above \n",
        "for implementing the functions. \n",
        "\"\"\"\n",
        "\n",
        "# Removes all the punctuations present in the document\n",
        "def remove_punctuation(doc):\n",
        "    # implement\n",
        "    \n",
        "    # comment the next line out\n",
        "    pass\n",
        "\n",
        "# Removes words like 'if', 'he', 'she', 'the', etc which never belongs to any topic\n",
        "def remove_stopwords(doc):\n",
        "    # implement\n",
        "\n",
        "    # comment the next line out\n",
        "    pass\n",
        "\n",
        "# lemmatizer is a transformers which transforms the word to its singular, present-tense form\n",
        "def lemmatize(doc):\n",
        "    # implement\n",
        "\n",
        "    # comment the next line out\n",
        "    pass\n",
        "\n",
        "def preprocess(doc):\n",
        "    \"\"\" Function to preprocess a single document\n",
        "    \"\"\"\n",
        "    assert isinstance(doc, str) # assert that input is a document and not the corpus\n",
        "    processed_doc = remove_punctuation(doc)\n",
        "    processed_doc = remove_stopwords(processed_doc)\n",
        "    processed_doc = lemmatize(processed_doc)\n",
        "    return processed_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRxDShbjAXjI"
      },
      "source": [
        "### Implementation of Naïve Bayes \n",
        "\n",
        "You can refer the Naïve Bayes section in [this](https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf) slides (slide #32 has a simple pseudo code) to get a hint about implementation of Naïve Bayes for text classification. Then complete the following functions `train_nb` and `classify_nb`.\n",
        "\n",
        "NOTE: If you multiply many small probabilities you may run into problems with numeric precision: the probability becomes zero. To handle this problem, it is recommended that you compute the logarithms of the probabilities instead of the probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz0ljnxZAfFX"
      },
      "source": [
        "def train_nb(training_documents):\n",
        "    # return the data you need to classify new instances\n",
        "\n",
        "    # comment the next line out\n",
        "    pass\n",
        "\n",
        "def classify_nb(training_documents):\n",
        "    # return the guess of the classifier\n",
        "\n",
        "    # comment the next line out\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slmjGSiuFPZv"
      },
      "source": [
        "### Train-test split\n",
        "After reading the dataset, you must split the dataset into training ($80\\%$) and test data ($20\\%$). Use training data to train the Naïve Bayes classifier and use test data to check the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llmwZsztFvkJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_DDcDPx_zKo"
      },
      "source": [
        "### Comparison (Bonus)\n",
        "Also use `sklearn`'s Naïve Bayes classifier and compare its performance with the classifier you implemented. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aexZ_3vAAPH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAngbx0yHvNN"
      },
      "source": [
        "Make sure your code is well documented with comments explaining everything done in your algorithm. With this being said, you are free to design your code anyway you like as long as it implements a Naïve Bayes model and is easily understandable. If you digress from the given code template, explain briefly the structure of your code as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBrANeu0aILI"
      },
      "source": [
        "# Problem 2: Regularization and bias-variance trade-off\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSph6_gW-ln0"
      },
      "source": [
        "### Problem statement\n",
        "In this question we will see how regularization can be used to prevent overfitting of data and then observe the bias-variance tradeoff in a practical setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQPYrvfLAnSY"
      },
      "source": [
        "### Dataset generation\n",
        "- Generate 10 data points $f(x)=sin(2\\pi x)$ where $x \\hspace{0.1cm} \\epsilon \\hspace{0.1cm} [0, 1]$ is sampled uniformly.\n",
        "- Add Gaussian noise $N(0, 0.5)$ to the generated data. By generating data in this way, we are capturing a property of many real data sets - namely, that they possess an underlying regularity $f(x)$, which we wish to learn, but that individual observations are corrupted by random noise $N(0,0.5)$.\n",
        "- We will now use this set of 10 data points as the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZcsltnkJCue"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjKUq1CjEXQG"
      },
      "source": [
        "### Polynomial curve fitting \n",
        "- Fit 5 polynomial regression models with varying polynomial orders $M = \\{0, 1, 3, 6, 9\\}$ on the training dataset. Use the polynomial function of the form:  $y(x, \\textbf{w})=\\sum^{M}_{j=0}w_jx^j$ and $L2$ loss as the error function: $E(\\textbf{w})= \\frac{1}{2}\\sum^{N}_{n=1}\\{y(x_n, \\textbf{w}) - t_n)\\}^2$, where $t_n$ is the true output for the input $x_n$, and $N$ is the total number of training points.\n",
        "- For each model: $M = \\{0, 1, 3, 6, 9\\}$, plot the graph of the function obtained from fitting the model onto the training dataset along with the training dataset points. \n",
        "- Report the mean squared error on the training dataset and explain its trend with increasing model complexity. Comment on overfitting and underfitting.\n",
        "- For each model: $M = \\{0, 1, 3, 6, 9\\}$, report the coefficients $\\textbf{w}^*$. Explain the trend in the coefficients with increasing model complexity. \n",
        "- The goal here is to achieve good generalization by making\n",
        "accurate predictions for new data, and not the training data. Use the data generation procedure used previously to generate 100 data points but with new choices for the random noise values included in the target values. These 100 data points will now form our validation dataset.\n",
        "- Evaluate each model: $M = \\{0, 1, 3, 6, 9\\}$ on the validation set and report the mean squared error for each model. \n",
        "- Plot the training and validation set mean squared errors for models with $M = \\{0, 1, 3, 6, 9\\}$ on the same graph. Explain the trend in the error values with increasing model complexity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnSZ2jAlMVCL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFM6HWgnACG8"
      },
      "source": [
        "### Regularization\n",
        "\n",
        "We've seen the effects of increasing model complexity on the training error and the validation error above. We will now use L2 regularization to reduce overfitting.\n",
        "\n",
        "- Fit a polynomial regression model of order $M=9$ to the same training dataset as before but now using the regularized error function given by $E^{'}(\\textbf{w})= \\frac{1}{2}\\sum^{N}_{n=1}\\{y(x_n, \\textbf{w}) - t_n)\\}^2 + \\frac{\\lambda}{2}{\\|\\textbf{w}\\|}^2$ where $\\lambda$ is the regularization hyperparameter. Use the following values for $\\lambda$: $\\lambda={0.01, 0.1, 1}$.\n",
        "- Report the coefficients of the model fit above for $\\lambda={0.01, 0.1, 1}$. Explain the trend in the coefficient values with increasing $\\lambda$.\n",
        "- Find the optimal value of the hyperparameter $\\lambda$. \n",
        "- Compare the validation error results of the following two models : polynomial regression model of order $M=9$ without regularization and polynomial regression model of order $M=9$ with regularization hyperparameter as estimated above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1FWv1E-MWVS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BwLU1odIi45"
      },
      "source": [
        "### Bias-variance trade-off:\n",
        "\n",
        "In class you have seen that the expected prediction error for any model can be decomposed as the sum of $bias^2, variance$ and $irreducible\\,noise$. We will now observe the bias-variance trade-off for a polynomial regression model of order $M=9$ with varying regularization hyperparameter.\n",
        "- Generate $50$ datasets, each containing $10$ points, independently, from the curve $f(x)=sin(2\\pi x)$. Add gaussian noise $N(0,0.5)$ to each data point.\n",
        "- Fit a polynomial regression model of order $M=9$ to each training dataset by minimizing the regularized error function $E^{'}(\\textbf{w})$ with $\\lambda=1$.\n",
        "- Plot the following:\n",
        "  - function obtained by training the model on each of the 50 datasets in the same figure.\n",
        "  - The corresponding average of the 50 fits and the sinusoidal function from which the datasets were generated in the same figure.\n",
        "- Repeat this exercise for two more $\\lambda$ values: $\\lambda$ = 0.1, 10.\n",
        "- Based on the plots obtained, explain the trend in the bias and variance values with increasing model complexity.\n",
        "- Bonus (optional and will not be graded) : \n",
        "  - Plot the $bias^2$, $variance$  and $bias^2 + variance$ against $\\lambda$.\n",
        "  - Also plot the average test error on a test data size of 1000 points (generated in a similiar way as the 50 training datasets, but independently) against $\\lambda$ on the same figure.\n",
        "  - For your reference: \n",
        "$$\n",
        "Bias^2= (E_{D}[\\hat f(x)] - f(x))^2\n",
        "\\\\\n",
        "Variance = E_{D}[(\\hat f(x) - E_{D}[\\hat f(x)])^2]\n",
        "$$\n",
        "Here $\\hat f$ is the trained model and $D$ is the set of all datasets. Use the $50$ training datasets to compute the empirical estimations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFYqiOEZNMuv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0akOo4P-EjhV"
      },
      "source": [
        "#Problem 3: Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL1ZH-z8YwuE"
      },
      "source": [
        "## Binary Logistic Regression\n",
        "\n",
        "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic **sigmoid function** $h_ \\theta (\\cdot)$ to return a probability value which can then be mapped to two or more discrete classes. $$ h_ \\theta (x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{- \\theta^Tx} }  $$ \n",
        "\n",
        "<br>here, the vector $\\theta$ represents the weights and the vector $x$ represents the given inputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxGXZkGXbnmJ"
      },
      "source": [
        "## Problem 3, Part A: Dataset A\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ndfkwa2WD58"
      },
      "source": [
        "\n",
        "Use Dataset A (``data_prob3_parta.csv``) for this part of the question. The given CSV file has three columns: column 1 is the first input feature, column 2 is the second input feature and column 3 is the output label. Split the dataset into training data (75%) and testing data (25%) randomly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiGqCdCYNfwN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmDzqj1JcUXJ"
      },
      "source": [
        "Visualize the training data with a scatter plot (input feature 1 on the X axis, input feature 2 on the Y axis and color the points according to their labels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khVm1XXAcUgX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeFBBGBIcUo0"
      },
      "source": [
        "Build the logistic regression model using the training data. \n",
        "\n",
        "The scikit library can be used to build the model. Bonus marks will be awarded if the model is built from scratch without using any external libraries. If you are writing your own implementation, try to keep number of features and number of classes as variables for next parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tf2Fu0sccUw_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvsbHz1kiv_Q"
      },
      "source": [
        "Print the final weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KUYqTxbiv_c"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FlYHEoOcU4l"
      },
      "source": [
        "Print the final accuracy on test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_xid_OfcVAq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVPSgR9fcVKR"
      },
      "source": [
        "Plot the scatter plot on test data. On top of this scatter plot, plot the decision boundaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcjT9KBbcVSG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCAFmjmFd2U0"
      },
      "source": [
        "## Problem 3, Part B: Dataset B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA_bKTy8eEgR"
      },
      "source": [
        "\n",
        "Use Dataset B (``data_prob3_partb.csv``) for this part of the question. The given CSV file has three columns: column 1 is the first input feature, column 2 is the second input feature and column 3 is the output label. Split the dataset into training data (75%) and testing data (25%) randomly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-yUXlVZeEgn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3ulzUFCeEhU"
      },
      "source": [
        "Visualize the training data with a scatter plot (input feature 1 on the X axis, input feature 2 on the Y axis and color the points according to their labels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzQpZl0beEhi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QOjHNDZeEiA"
      },
      "source": [
        "Build the logistic regression model using the training data. The scikit library can be used to build the model. Bonus marks will be awarded if the model is built from scratch without using any external libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHT1ErZzeEiM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRv3J0eXiuyU"
      },
      "source": [
        "Print the final weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3_bJwqLiuyi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s7BCmFueEik"
      },
      "source": [
        "Print the final accuracy on test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQUaA0OPeEis"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3mccmiHeEi8"
      },
      "source": [
        "Plot the scatter plot on test data. On top of this scatter plot, plot the decision boundaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et2I-cU5eEjH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNOuPeBuemS2"
      },
      "source": [
        "As you can see, a straight line is not the best decision boundary for this type of data. In the next part, we will try polynomial feature mapping to generate more features and build the classifier on top of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciF6EYBIeciU"
      },
      "source": [
        "## Problem 3, Part C: Polynomial Feature Mapping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdqHppFWecis"
      },
      "source": [
        "\n",
        "Use Dataset B (``data_prob3_partb.csv``) for this part of the question.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A7KCyZYecjB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLzYBYAecjy"
      },
      "source": [
        "Generate more features for each datapoint using the following transformation.\n",
        "\n",
        "For a datapoint $(x_1, x_2)$,\n",
        "$$ (x_1, x_2) \\rightarrow (x_1, x_2, x_1^2, x_2^2, x_1^3, x_2^3, ..., x_1^T, x_2^T) $$\n",
        "Now, instead of giving $(x_1, x_2)$ as the input to the classifier, use the transformed data as the input to the classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XraF1LYgGic"
      },
      "source": [
        "Generate the transformed training and testing dataset using Dataset B (``data_prob3_partb.csv``)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UG8Aa37eckB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq7lZvceeckj"
      },
      "source": [
        "Build the logistic regression model using the transformed training data. The scikit library can be used to build the model. Bonus marks will be awarded if the model is built from scratch without using any external libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjx4dS_Necks"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx0chW1jgxUO"
      },
      "source": [
        "Try different values of $T$ (highest number of degree) between 3 to 10. Find out which value of $T$ gives the best test accuracy. Please print that values of $T$ in the below cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpogT2Eugxdu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v35BcgyzitzF"
      },
      "source": [
        "Print the final weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQRATqLLitzi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9wbRysyeclC"
      },
      "source": [
        "Print the final accuracy on transformed test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nljp_btdeclI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZw-H4Wuecle"
      },
      "source": [
        "Plot the scatter plot on test data (note that this is  the original data , not the transformed one). On top of this scatter plot, plot the new decision boundaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6btn-cxDeclk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7j1dg8ibMzr"
      },
      "source": [
        "## Problem 3, Part D: Multi-class Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTt_8MS4Zm03"
      },
      "source": [
        "## Multi-class Logistic Regression\n",
        "\n",
        "In case of a multi-class classification problem (when the number of classes is greater than two), a **softmax function** is used instead. \n",
        "$$\\text{Softmax}(\\theta_{i}) = \\frac{\\exp(\\theta_i)}{\\sum_{j=1}^{N} \\exp(\\theta_j)}$$ where $j$ varies from $1$ to $N$ which is the number of classes and  $\\theta_{i}$ is $$\\theta_{i}=W_{i}*x^{(i)}+b$$ Where $x^{(i)}$ is a feature  vector of dimensions $D \\times 1$ and $W_{i}$ is the $i$-th row of the weight matrix $ W$ of  dimensions $N \\times D$  and $b$ is the bias having dimensions $D \\times 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "485Jjh7CiWpz"
      },
      "source": [
        "\n",
        "Use Dataset D (``data_prob3_partd.csv``) for this part of the question. The given CSV file has three columns: column 1 is the first input feature, column 2 is the second input feature and column 3 is the output label. Split the dataset into training data (75%) and testing data (25%) randomly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxWD-mjQiWp_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIImFy8DiWqi"
      },
      "source": [
        "Visualize the training data with a scatter plot (input feature 1 on the X axis, input feature 2 on the Y axis and color the points according to their labels)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wSbHk9SiWqt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERqEDKMsiWrM"
      },
      "source": [
        "Build the logistic regression model using the training data. The scikit library can be used to build the model. Bonus marks will be awarded if the model is built from scratch without using any external libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B11GMS_niWrY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWzOaNrkimv3"
      },
      "source": [
        "Print the final weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAUpUwbZim2X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UucCk_-iWry"
      },
      "source": [
        "Print the final accuracy on test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB6wvccOiWr8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnTHrX6hiWsT"
      },
      "source": [
        "Plot the scatter plot on test data. On top of this scatter plot, plot the decision boundaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuBkU27yiWsb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRzDZxIhn76D"
      },
      "source": [
        "# Problem 4: Learning on real world datasets\n",
        "\n",
        "*Cric_data.csv* contains the batting averages and bowling averages of various cricket players along with their role in the team (Bowler/Batsman/Allrounder). The task is to predict the player role based on their batting and bowling averages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-NWN2KyoW6K"
      },
      "source": [
        "In the next CodeWrite cell, extract the required columns from the csv file, partition the data into training (75%) and testing (25%) data randomly.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww4f4B90oW6T"
      },
      "source": [
        "# Extract data and partition\n",
        "\n",
        "# import pandas as pd\n",
        "# inp = pd.read_csv('Cric_data.csv', usecols=['Batting Average', 'Bowling Average', 'Player Class']).values\n",
        "\n",
        "# X_train = \n",
        "# X_test = \n",
        "# Y_train = \n",
        "# Y_test = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5ED_HDQoW6l"
      },
      "source": [
        "**Binary Classification:**\n",
        "\n",
        "Derive the classifiers under the assumptions below, and use ML estimators to compute and return the results on the test set. *Consider only batsmen and bowlers in this part*.\n",
        "\n",
        "Let random variable $\\underline X$ represent (Batting Average, Bowling Average) of a player whose role is a random variable $Y$.\n",
        "\n",
        "1a) Linear Predictor: Assume $\\underline X|Y=Batsman \\sim \\mathcal{N}(\\underline {\\mu_-}, I)$ and  $X|Y=Bowler \\sim \\mathcal{N}(\\underline {\\mu_+}, I)$. \n",
        "\n",
        "1b) Bayes Classifier: Assume $\\underline X|Y=Batsman \\sim \\mathcal{N}(\\underline {\\mu_-}, \\Sigma_-)$ and  $X|Y=Bowler \\sim \\mathcal{N}(\\underline {\\mu_+}, \\Sigma_+)$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFFcaecYoW6t"
      },
      "source": [
        "def Lin_clsf_1a(X_train, Y_train, X_test):\n",
        "    \"\"\" Give prediction for test data using assumption 1a.\n",
        "\n",
        "    Arguments:\n",
        "    X_train: numpy array of shape (n,2)\n",
        "    Y_train: +1/-1 numpy array of shape (n,)\n",
        "    X_test : numpy array of shape (m,2)\n",
        "\n",
        "    Returns:\n",
        "    Y_test_pred : +1/-1 numpy array of shape (m,)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Bayes_clsf_1b(X_train, Y_train, X_test):\n",
        "    \"\"\" Give prediction for test data using assumption 1b.\n",
        "\n",
        "    Arguments:\n",
        "    X_train: numpy array of shape (n,2)\n",
        "    Y_train: +1/-1 numpy array of shape (n,)\n",
        "    X_test : numpy array of shape (m,2)\n",
        "\n",
        "    Returns:\n",
        "    Y_test_pred : +1/-1 numpy array of shape (m,)\n",
        "\n",
        "    \"\"\"\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbkxj8euoW65"
      },
      "source": [
        "**Multi-class Classification:**\n",
        "\n",
        "Derive the classifiers under the assumptions below, and use ML estimators to compute and return the results on the test set. *Consider batsmen, bowlers and allrounders in this part*.\n",
        "\n",
        "Let random variable $\\underline X$ represent (Batting Average, Bowling Average) of a player whose role is a random variable $Y$.\n",
        "\n",
        "The $3\\times 3$ loss matrix giving the loss incurred for predicting $i$ when truth is $j$ is below. (Ordering: Batsman - Allrounder - Bowler)\n",
        "\n",
        "$L=\\begin{bmatrix} 0 &1 & 2\\\\ 1 &0 & 1\\\\ 2 &1 & 0\\end{bmatrix}$ \n",
        "\n",
        "2a) Linear Predictor: Assume $\\underline X|Y=a \\sim \\mathcal{N}(\\underline {\\mu_a}, I)$\n",
        "\n",
        "2b) Bayes Classifier: Assume $\\underline X|Y=a \\sim \\mathcal{N}(\\underline {\\mu_a}, \\Sigma_a)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxKx5gBfoW69"
      },
      "source": [
        "def Lin_clsf_2a(X_train, Y_train, X_test):\n",
        "    \"\"\" Give prediction for test data using assumption 2a.\n",
        "\n",
        "    Arguments:\n",
        "    X_train: numpy array of shape (n,2)\n",
        "    Y_train: +1/0/-1 numpy array of shape (n,)\n",
        "    X_test : numpy array of shape (m,2)\n",
        "\n",
        "    Returns:\n",
        "    Y_test_pred : +1/0/-1 numpy array of shape (m,)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Bayes_clsf_2b(X_train, Y_train, X_test):\n",
        "    \"\"\" Give prediction for test data using assumption 2b.\n",
        "\n",
        "    Arguments:\n",
        "    X_train: numpy array of shape (n,2)\n",
        "    Y_train: +1/0/-1 numpy array of shape (n,)\n",
        "    X_test : numpy array of shape (m,2)\n",
        "\n",
        "    Returns:\n",
        "    Y_test_pred : +1/0/-1 numpy array of shape (m,)\n",
        "\n",
        "    \"\"\"\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Isp0HQWroW7J"
      },
      "source": [
        "**Plots:**\n",
        "\n",
        "In the next CodeWrite cell, plot all the 4 classifiers on a 2d plot. Take a suitable grid covering averages (0,60) in both dimensions. (Color the different classes accordingly). Add the training data points also on the plot. Label the plots appropriately. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32FoBR6WoW7L"
      },
      "source": [
        "# Write the code for plotting here. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fyQx73JoW7S"
      },
      "source": [
        "**Observations:**\n",
        "\n",
        "In the next Textwrite cell, summarise (use the plots of the data and the assumptions in the problem to explain) your observations regarding the four learnt classifiers, and also give the error rate of the four classifiers as a 2x2 table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP7EhbdpoW7V"
      },
      "source": [
        "** Cell type : TextWrite ** \n",
        "(Write your observations and table of errors here)"
      ]
    }
  ]
}